{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "separated-audience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "print (brown.categories())\n",
    "data = brown.sents(categories = \"news\")\n",
    "print(' '.join(data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-cyprus",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indian-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\srish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpine-smile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nWe are having fun at major league hacking local hack day build!',\n",
       " 'feel free to join the discord channel.',\n",
       " 'also share about the session on social media.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "document = \"\"\"\n",
    "We are having fun at major league hacking local hack day build! \n",
    "feel free to join the discord channel. also share about the session on social media.\n",
    "\"\"\"\n",
    "\n",
    "sentence = \"send all the documents related to chapter 1,2,3,4 to kunal@xyz.com\"\n",
    "sents = sent_tokenize(document)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acute-denver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send',\n",
       " 'all',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapter',\n",
       " '1,2,3,4',\n",
       " 'to',\n",
       " 'kunal',\n",
       " '@',\n",
       " 'xyz.com']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-division",
   "metadata": {},
   "source": [
    "# Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eight-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "simple-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\srish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "infinite-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "checked-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    useful_words = [word for word in text if word not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "demanding-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'documents', 'related', 'chapter', '1,2,3,4', 'kunal@xyz.com']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(sentence.split(), sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "finite-spine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['send', 'all', 'the', 'documents', 'related', 'to', 'chapter', 'to', 'kunal@xyz.com']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]+')\n",
    "useful_text = tokenizer.tokenize(sentence)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-essex",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tropical-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"\n",
    "I like this session and I liked all the sessions. I am liking everything. \n",
    "We are having fun at major league hacking local hack day build! \n",
    "feel free to join the discord channel. also share about the session on social media.\n",
    "\"\"\"\n",
    "\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "british-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "blessed-parking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('liking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "varying-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Indian cricket team will win World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "    'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "    'The nobel laurate won the hearts of the people',\n",
    "    'The movie Raazi is an exciting Indian Spy thriller based upon a real story'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "distributed-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (8.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\srish\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "standard-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "productive-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "infrared-distance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
      " 2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vectorized_corpus = vectorized_corpus.toarray()\n",
    "print(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adequate-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 12, 'cricket': 6, 'team': 31, 'will': 37, 'win': 38, 'world': 40, 'cup': 7, 'says': 27, 'capt': 4, 'virat': 35, 'kohli': 14, 'be': 3, 'held': 11, 'at': 1, 'sri': 29, 'lanka': 15, 'we': 36, 'next': 19, 'lok': 17, 'sabha': 26, 'elections': 8, 'confident': 5, 'pm': 23, 'the': 32, 'nobel': 20, 'laurate': 16, 'won': 39, 'hearts': 10, 'of': 21, 'people': 22, 'movie': 18, 'raazi': 24, 'is': 13, 'an': 0, 'exciting': 9, 'spy': 28, 'thriller': 33, 'based': 2, 'upon': 34, 'real': 25, 'story': 30}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-serial",
   "metadata": {},
   "source": [
    "# Vectorisation with stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "twenty-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(document):\n",
    "    words = tokenizer.tokenize(document.lower())\n",
    "    words = remove_stopwords(words, sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "filled-pharmacology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['function']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer('this is some function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "generic-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer = myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "prescribed-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizedCorpus = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "committed-radiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizedCorpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "creative-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 9, 'cricket': 3, 'team': 26, 'win': 30, 'world': 31, 'cup': 4, 'says': 22, 'capt.': 1, 'virat': 29, 'kohli.': 10, 'held': 8, 'sri': 24, 'lanka.': 11, 'next': 15, 'lok': 13, 'sabha': 21, 'elections': 5, 'confident': 2, 'pm': 18, 'nobel': 16, 'laurate': 12, 'hearts': 7, 'people': 17, 'movie': 14, 'raazi': 19, 'exciting': 6, 'spy': 23, 'thriller': 27, 'based': 0, 'upon': 28, 'real': 20, 'story': 25}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-trunk",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
